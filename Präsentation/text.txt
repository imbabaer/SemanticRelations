Motivation und Problemstellung
Eine Aufgabe des NLP ist es, Beziehungen zwischen Wörtern darzustellen. Um solche Beziehungen darzustellen gibt es unterschiedliche Techniken bzw. Vorgehgensweisen. Neben z.B. tf-idf können semantische Beziehungen auch mit Word2Vec dargestellt werden. Und um Word2Vec soll es in dieser Arbeit auch gehen. Es soll untersucht werden, wie sich unterschiedliche Textgrundlagen (Korpora) als Trainingsdaten auf die Beziehungen zwischen den Wörtern im Modell auswirken. Die Ergebnisse der Arbeit sollen es ermöglichen, dass für eine gewisse Fragestellung oder Anwendung des Word2Vec Modells, die richtigen Trainingsdaten ausgewählt werden können.

Word2Vec


Gensim-Implementierung


Daten und Vorverarbeitung
Für das Training von Word2Vec Modellen wird eine große Anzahl an Trainingsdaten benötigt. Eine große Anzahl bedeutet hier, im Optimalfall mehrere Milliarden von Wörtern. Auf der Googlecode-Seite von Word2Vec werden online verfügbare Datensätze aufgelistet. Unter anderem auch der komplette Wikipediakorpus und ein kleinerer Teilkorpus mit den ersten 1 Milliarde Zeichen. In dieser Arbeit sollen Modelle mit unterschiedlichen Trainingskorpora verglichen werden, also müssen diese unterschiedlichen Korpora erstellt werden. Die Wahl fiel dann zum einen auf den gesamten Wikipediakorpus und zum anderen auf einen domänenspezifischen Teilkorpus, der aus allen Artikeln des Gesamtkorpus besteht, die sich mit der Thematik Technologie/Pc/Internet befassen. Um diesen Teilkorpus zu erhalten, waren eingige Vorverarbeitungsschritte nötig. Der komplette Wikipediakorpus kann als große XML-Datei heruntergeladen werden. Um aus dieser Datei die einzelnen Artikel zu extrahieren, muss sie mit einem SAX-Parser geparst werden. Wenn dann die einzelnen Artikel als separate Dateien vorliegen, können diese Klassifiziert werden. Für die Klassifizierung wurde der, im DataMining-Praktikum erstellte, Naive Bayes Klassifikator benutzt. Um ihn zu trainieren mussten zuerst von Hand Artikel getaggt werden. Für jede Klasse sollten 100 Artikel für das Training des NBCs gefunden werden. Und bei den fünf Klassen (tech, entertainment, sport, science, politic) waren das dann 500 Artikel die gefunden werden mussten. Das hat einige Zeit in Anspruch genommen, da oft die Wikipediaartikel nicht eindeutig zuordenbar waren und ich sie dann lieber nicht für das Training verwendet habe. Nach der Klassifizierung wurden dann alle Artikel mit einem Perlskript von Matt Mahoney, das leicht angepasst wurde, vom Wiki-Markup gereinigt. Dieses Skript wurde auch auf die große XML-Datei angewandt um den Gesamtkorpus zu reinigen. Nachdem die einzelnen Artikel gereinigt waren, wurden sie wieder zusammengefügt. Die Word2Vec-Implementierung in Gensim sieht es vor, dass die Trainingsdaten als Array aus einzelnen Sätzen eingegeben werden. Da die Datei mit dem kompletten Wikipediakorpus sehr groß ist, kann sie zwar direkt in Python geöffnet werden, allerdings wird das Array mit den einzelnen Sätzen zu groß (bei über 40 GB habe ich dann abgebrochen). Die Word2Vec Klasse kann alternativ zu dem Array aus Sätzen auch ein Objekt der Klasse "gensim.models.word2vec.LineSentence" entgegen nehmen. In der Input-Datei dieser Klasse müssen die einzelnen Zeilen je einen Satz enthalten. So habe ich dann den Gesamtkorpus zuerst von Satzzeichen und Großschreibung befreit und dann die einzelnen Sätze in je eine Zeile gebracht. Damit konnte ich dann mittels der erwähnten "LineSentence" Klasse das Word2Vec Modell erstellen und trainieren. Beim Technologiekorpus bin ich dann gleich vorgegangen. Theoretisch wäre diese Datei nicht so groß gewesen und man hätte einfach ein Array mit den einzelnen Sätzen erstellen können, da ich die Skripte ja schon hatte habe ich einfach die gleiche Vorangehensweise benutzt. 


Wikipedia Korpus, verwendete Modellparameter und Testdaten
Die Wikipedia Daten wurden aus mehreren Gründen benutzt. Erstens ist die Menge der Daten sehr groß, was für eine gute Qualität der Wortvektoren im Word2Vec Modell wichtig ist. Im kompletten Korpus sind es über 2,9 Milliarden an Wörtern. Zweitens werden in Wikipedia eine sehr breite Menge an Themen beschrieben, was für das allgemeine Modell sehr hilfreich ist.
Da die Skip-gram Architektur im Bezug auf semantsiche Ähnlichkeit deutlich besser ist, als die CBOW Architektur, wurde diese Architektur ausgewählt, da in den Experimenten semantische Ähnlichkeiten überprüft werden sollen. Beim Lernalgorithmus des Neuronalen Netzs fiel die Wahl auf den hierarchical softmax Algorithmus, da dieser besser für selten vorkommende Wörter geeignet ist. Und so für die technologiespezifischen Daten besser passt. Zudem ist der negative sampling Lernalgorithmus eher geeignet für niedrigdimensionale Vektoren, und wie wir nacher noch sehen, werden eher hochdimensionale Wortvektoren benützt.
Um die weiteren Parameter für das Training des Word2Vec Modells herauszufinden, wurde der kleinere Wikipedia Auszug mit den ersten 1 Milliarde Zeichen als Trainingsdaten benutzt. Es wurden dann mehrere unterschiedliche Modelle mit unterschiedlichen Parametern erstellt und verglichen. Der Vergleich wurde mit der in der Gensim-Implementierung enthaltenen Accuracy()-Funktion durchgeführt. Die Funktion erwartet einen Dateinamen einer Datei, in der jede Zeile aus zwei Wortpaaren besteht, die Worte der Paare haben die gleiche Beziehung untereinander. Zum Beispiel "Athens Greece Berlin Germany", hier stellen die Wortpaare die Beziehung zwischen der Hauptstadt und dem dazugehörigen Land dar. Diese Datei kann noch in Abschnitte unterteilt werden, was mit ": SECTION NAME" erreicht werden kann. Auf der Google Code Seite von Word2Vec ist eine solche Datei als Beispiel vorhanden, diese enthält 19544 solcher 4-Tupel und ist in 14 Kategorien eingeteilt( capital-common-countries, capital-world, currency, city-in-state, family, gram1-adjective-to-adverb, gram2-opposite, gram3-comparative, gram4-superlative, gram5-present-participle, gram6-nationality-adjective, gram7-past-tense, gram8-plural, gram9-plural-verbs). In der Gensim-Implementierung stellt das Modell eine Funktion bereit, die zwei Arrays und eine Zahl "N" entgegennimmt, das eine ist das "positive-Array", das andere das "negative-Array". In dieser Funktion werden die Wortvektoren der Worte im Array positive aufaddiert und die Vektoren aus dem negative-Array davon subtrahiert. Die Funktion liefert dann N Worte zurück, die eine maximale Kosinusähnlichkeit mit dem Ergebnis der Vektoraddition und -subtraktion haben. Diese Funktion wird in der Accuracy-Funktion benutzt, indem die ersten drei Worte des 4-Tupels in positive und negative eingeteilt werden und vierte Wort muss dann korrekt vom Model vorhergesagt werden. Hier: most similar(positive=['Greece','Berlin'], negative=['Athens'], topn=1). 
Tabelle 4.1 einfügen.
Diese Werte beziehen sich auf den kleinen Wikipediakorpus. Mit den Parametern size=400, Window=10 und Min_count=5 wurde die beste Accuracy erreicht. Mit diesen Parametern beträgt die Trainingsdauer des Gesamtmodells über 10,5h. Durch einen Fehler im Perl-Reinigungsskript musste das Modell im nachhinein nochmals trainiert werden, hier fiel dann auch die Entscheidung, andere Parameter zu verwenden und so wurde size auf 300 verringert. Hier betrug die Trainingszeit dann nur noch 7,7h. Da sich das Perl-Skript noch wenige Male geringfügig änderte, war diese Entscheidung gut.




Experimente


Fazit und Ausblick


